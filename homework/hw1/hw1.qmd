---
title: "DATA 221 Homework 1"
author: "Chris Low"
format: pdf
jupyter: python3
---


# Problem 1

## 1(a)

For each 30 minute interval,

$$
P(X_i = x_i \mid \lambda) \;=\; e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}, 
\qquad x_i \ge 0.
$$

Assuming independence across the 16 intervals, the likelihood is the joint probability:
$$
L(\lambda \mid x_1,\dots,x_{16})
\;=\;
\prod_{i=1}^{16} P(X_i = x_i \mid \lambda)
\;=\;
\prod_{i=1}^{16}\left(e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\right).
$$

Collect terms:
$$
\prod_{i=1}^{16} e^{-\lambda} = e^{-16\lambda},
\qquad
\prod_{i=1}^{16} \lambda^{x_i} = \lambda^{\sum_{i=1}^{16} x_i},
\qquad
\prod_{i=1}^{16} \frac{1}{x_i!} = \frac{1}{\prod_{i=1}^{16} x_i!}.
$$

Therefore,
$$
\boxed{
L(\lambda \mid x_1,\dots,x_{16})
=
e^{-16\lambda}\,
\frac{\lambda^{\sum_{i=1}^{16} x_i}}{\prod_{i=1}^{16} x_i!}
}.
$$


## 1(b)

From 1(a),
$$
L(\lambda \mid x_1,\dots,x_{16})
=
e^{-16\lambda}\,
\frac{\lambda^{\sum_{i=1}^{16} x_i}}{\prod_{i=1}^{16} x_i!}.
$$

Take logs:
$$
\ell(\lambda)
=
\log L(\lambda \mid x_1,\dots,x_{16})
=
\log\left(e^{-16\lambda}\right)
+
\log\left(\lambda^{\sum_{i=1}^{16} x_i}\right)
-
\log\left(\prod_{i=1}^{16} x_i!\right).
$$

Simplify:
$$
\log\left(e^{-16\lambda}\right) = -16\lambda,
\qquad
\log\left(\lambda^{\sum x_i}\right)=\left(\sum_{i=1}^{16} x_i\right)\log \lambda,
\qquad
\log\left(\prod x_i!\right)=\sum_{i=1}^{16}\log(x_i!).
$$

Therefore,
$$
\boxed{
\ell(\lambda)
=
-16\lambda
+
\left(\sum_{i=1}^{16} x_i\right)\log \lambda
-
\sum_{i=1}^{16}\log(x_i!)
}.
$$

## 1(c)

Differentiate:
$$
\frac{d}{d\lambda}\ell(\lambda)
=
-16
+
\left(\sum_{i=1}^{16} x_i\right)\frac{1}{\lambda}.
$$

Set to zero and solve:
$$
-16 + \frac{\sum_{i=1}^{16} x_i}{\lambda} = 0
\;\Rightarrow\;
\hat{\lambda}
=
\frac{1}{16}\sum_{i=1}^{16} x_i.
$$

Second derivative:
$$
\ell''(\lambda)
=
-\frac{\sum_{i=1}^{16} x_i}{\lambda^2}
<0 \quad \text{for } \lambda>0,
$$

So we know that $\ell'(\lambda)$ is concave, and thus the critical point is a maximizer of the log-likelihood.

\newpage


# Problem 2


## 2(a)

From Problem 1,
$$
\hat{\lambda}=\frac{1}{16}\sum_{i=1}^{16}x_i.
$$

Here,
$$
\sum_{i=1}^{16}x_i = 442
\quad\Rightarrow\quad
\hat{\lambda}=\frac{442}{16}=\boxed{27.625}.
$$



## 2(b)

Define the negative log-likelihood for Poisson data (from 1(b)):

$$
\text{NLL}(\lambda)
=
-\sum_{i=1}^{16}\log\left(e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\right)
=
\sum_{i=1}^{16}\left(\lambda - x_i\log\lambda + \log(x_i!)\right).
$$

Minimizing this function over $\lambda>0$ gives us the MLE.

```{python}
import numpy as np
from scipy.optimize import minimize_scalar
from scipy.special import gammaln

x = np.array([28, 33, 21, 27, 24, 35, 26, 30, 18, 29, 31, 22, 34, 25, 27, 32], dtype=float)

def neg_log_likelihood(lam):
    if lam <= 0:
        return float("inf")
    # log-likelihood: sum(-lam + x_i log(lam) - log(x_i!))
    log_like = np.sum(-lam + x * np.log(lam) - gammaln(x + 1))
    return -log_like

result = minimize_scalar(neg_log_likelihood, bounds=(1e-8, 200), method="bounded")
lam_optimizer = result.x
lam_formula = np.mean(x)

print(lam_formula)
print(lam_optimizer)
```

The optimizer returns $\hat\lambda \approx 27.625$, which matches our closed-form estimate from part (a).



## 2(c)


We have a Poisson rate $\lambda$ per 30 minutes. One hour is two consecutive 
30 minute calls. We let $X_{1}$ be calls in the first half hour and 
let $X_{2}$ be calls in the second half hour. The model assumes that $X_{1}$ and $X_{2}$ are independent and each is $\text{Poisson}(\hat\lambda)$.

$$
Y = X_{1} + X_{2} \sim \text{Poisson}(2\hat\lambda).
$$

$\hat\lambda = 27.625$, so we have $2\hat\lambda = 55.25$. T

$$
P(Y \ge 60) = 1 - P(Y \leq 59) = 
1 - \sum_{k=0}^{59} e^{-55.25}\frac{55.25^k}{k!}.
$$


```{python}
from scipy.stats import poisson

mu = 2 * result.x
print(poisson.sf(59, mu))
```

$$
P(Y \ge 60) \approx 0.279.
$$


\newpage


# Problem 3

## 3(a)

The possible outcomes summing two dice to 6 are
$$
(1,5), (2,4), (3,3), (4,2), (5,1)
$$

which are equally likely.

Only one of these outcomes has the first die equal to 2, which is $(2,4)$. Therefore,
$$
P(\text{first die} = 2 \mid \text{sum} = 6) = \boxed{\frac{1}{5}}
$$

## 3(b)

By Bayesâ€™ rule,
$$
P(P \mid T)
=
\frac{P(T \mid P)P(P)}{P(T)}.
$$

Substituting the given values,
$$
P(P \mid T)
=
\frac{(0.30)(0.55)}{0.45}
=
\frac{11}{30}
\approx \boxed{0.367}
$$



# Problem 4

```{python}
import numpy as np

# Define the face probabilities (counts / 24)
faces = np.arange(1, 13)
# Die B counts from table
probs_b = np.array([3, 3, 3, 3, 3, 2, 2, 2, 1, 1, 0, 1]) / 24.0
# Die C counts from table (all 2)
probs_c = np.array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) / 24.0

def run_simulation(true_die_probs, target_odds=99):
    log_odds = 0  # log(1) = 0 starts at equal probability
    rolls = 0
    # log(99) is the threshold. 
    # Log-odds form: log(P(B)/P(C)) = Sum(log(P(x|B)) - log(P(x|C)))
    threshold = np.log(target_odds)
    
    while abs(log_odds) < threshold:
        # Roll the die
        roll = np.random.choice(faces, p=true_die_probs)
        idx = roll - 1
        
        # Update Log Odds (using log prevents underflow)
        # Avoid log(0) by handling the 0-prob case for Die B if necessary
        # However, if we roll using Die B, we won't roll an 11 (prob 0), so it's safe.
        term_b = probs_b[idx]
        term_c = probs_c[idx]
        
        # If the model assigns 0 prob to an outcome that happened, odds explode
        if term_b == 0: 
            log_odds = -np.inf # B is impossible
        elif term_c == 0:
            log_odds = np.inf # C is impossible
        else:
            log_odds += np.log(term_b) - np.log(term_c)
            
        rolls += 1
        
    return rolls

# Run Monte Carlo
n_sims = 10000
rolls_given_b = [run_simulation(probs_b) for _ in range(n_sims)]
rolls_given_c = [run_simulation(probs_c) for _ in range(n_sims)]

print(f"Avg rolls given we are rolling B: {np.mean(rolls_given_b):.2f}")
print(f"Avg rolls given we are rolling C: {np.mean(rolls_given_c):.2f}")
```