---
title: "DATA 221 Homework 5"
author: "Chris Low"
format: pdf
execute:
  engine: python
  python: /Users/chrislowzx/miniconda3/bin/python
---


# Problem 1

### Exploratory Data Analysis
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

epi = pd.read_csv("epi_r.csv", index_col='title')

# EDA
print(f"Dataset shape: {epi.shape}")
print(epi.info())

nutritional_vars = ['calories', 'protein', 'fat', 'sodium']
print(epi[nutritional_vars].describe())

# Plotting to see the extreme outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=epi[nutritional_vars])
plt.title("Nutritional Variables (Notice the extreme outliers)")
plt.yscale('log') # Log scale: outliers are massive
plt.show()

# Check for missing values
print("Missing values before imputation:\n", epi[nutritional_vars].isna().sum())
```


I decided to pre-process mainly nutritional variables because they are numerical and directly relevant to the analysis we will be doing (PCA and regularized logistic regression). These variables are likely to have a significant impact on the outcome variable 'cake', which is what we will be trying to predict.

The other features, such as Inredients, are not numerical and would require different pre-processing steps (like encoding) if we were to include them in the analysis. For the scope of this homework, we will focus on the nutritional variables as features for PCA and regularized logistic regression, and 'cake' as the outcome variable.


Some pre-processing steps I will take include:

1. Handling Missing Values: I will impute missing values using the median, as it is more robust to outliers than the mean.

2. Handling Outliers: I will cap the values at the 99th percentile to mitigate the influence of extreme outliers without dropping too much data.

3. Scaling the Features: I will standardize the features to have a mean of 0 and a standard deviation of 1, which is crucial for PCA and regularized logistic regression to perform well.

```{python}
# PRE-PROCESSING

# Handle Missing Values (Median: as it is more robust to outliers than mean)
for col in nutritional_vars:
    epi[col] = epi[col].fillna(epi[col].median())

# Handle Outliers (Capping at the 99th percentile). We do this so we don't
# drop too much data, but we eliminate non-sensical values that are likely
# data entry errors or extreme outliers that could skew our analysis.
for col in nutritional_vars:
    cap_value = epi[col].quantile(0.99)
    epi[col] = np.where(epi[col] > cap_value, cap_value, epi[col])

epi[nutritional_vars].skew()

for col in nutritional_vars:
    print(f"{col} skew before log:", epi[col].skew())

    log_col = np.log1p(epi[col])
    print(f"{col} skew after log:", log_col.skew())
    print()
```

From the above code, the nutritional variables still show strong right skew ($\text{skewness} \approx 3$) after capping the 99 percentile. After applying a $log(1 + x)$ transformation, skewness decreased a lot toward zero, so this shows more symmetric distribution. This log transformation prior to scaling improves the stability of PCA.

Moving onto the next steps, where are remove binary tags that are too rare (less than 20 occurrences) (because they are not informative for modeling) and then separate the outcome variable 'cake' from the features, and finally scale the features for PCA and regularized logistic regression.

```{python}
# Remove rare binary tags
binary_cols = [col for col in epi.columns 
               if set(epi[col].unique()).issubset({0,1})]

min_count = 20
rare_cols = [col for col in binary_cols 
             if epi[col].sum() < min_count]

epi = epi.drop(columns=rare_cols)

# Separate outcome (i.e. cake) from features
y = epi['cake']
X = epi.drop(columns=['cake'])

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```


## 1(a) 
```{python}
from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X_scaled)
```


## 1(b)
```{python}
# Transpose because pca.components_ has shape (n_components, n_features),
# and we want (n_features, n_components) to align with our original features
loadings = pca.components_.T

pc1 = loadings[:, 0]
pc2 = loadings[:, 1]

plt.figure(figsize=(12,8))
plt.scatter(pc1, pc2, alpha=0.5)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Loadings: PC1 vs PC2')

# Label strongest contributors to PC1 and PC2 (absolute loading > 0.1)
for i, feature in enumerate(X.columns):
    if abs(pc1[i]) > 0.1 or abs(pc2[i]) > 0.1:
        plt.text(pc1[i], pc2[i], feature, fontsize=8)

plt.show()
```


**Patterns:** 


In the scatter plot of PC1 vs PC2, **most ingredient tags are clustered near the origin**, which suggests they do not strongly influence the first two principal components. This makes sense because many tags are sparse and do not vary consistently across recipes, so they do not drive the main directions of variation.

**The nutritional variables (calories, fat, sodium, and protein) appear grouped together in the lower-left region.** This suggests that one of the components is capturing something like overall richness or heaviness, since these variables tend to increase together. We also see that **alcohol (wine, beer, liquor) tags cluster together in the upper-left area**, which may indicate that they contribute to a different dimension of variation related to alcoholic content or recipe type.

**On the positive side of PC1, dietary restriction tags such as vegetarian, vegan, gluten-free, and dairy-free cluster together,** which suggests that PC1 may represent a dietary or health-conscious dimension.

**Dessert stands out in the upper-right area of the plot,** indicating that it contributes differently from both the nutritional variables and alcohol-related tags. 

Overall, the plot shows clear groupings of related features, showing that the first two principal components capture meaningful structure in the dataset rather than random noise.


## 1(c)
```{python}
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

print("Number of components:", len(pca.explained_variance_ratio_))
print("First 10 explained variances:")
print(explained_variance[:10])

print("Cumulative variance (first 10):")
print(cumulative_variance[:10])

print("Explained by PC1:", explained_variance[0])
print("Explained by PC2:", explained_variance[1])
print("Total explained by PC1 + PC2:", explained_variance[0] + explained_variance[1])
```


```{python}
plt.figure(figsize=(8,5))
plt.plot(cumulative_variance)
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Variance Explained")
plt.title("Cumulative Variance Explained by PCA")
plt.grid(alpha=0.3)
plt.show()
```

Scree plot: 
```{python}
plt.figure(figsize=(8,5))

plt.plot(range(1, len(pca.explained_variance_) + 1),
         pca.explained_variance_,
         marker='o')

plt.xlabel("Principal Component")
plt.ylabel("Eigenvalue")
plt.title("Scree Plot")
plt.grid(alpha=0.3)

plt.show()

```


Based on the scree plot, I cannot see a clear "elbow". However, there is a noticeable drop in eigenvalues after the first few components. I decided to use the \href{https://kneed.readthedocs.io/en/stable/}{KneeLocator} method to identify the optimal number of components. 

```{python}
from kneed import KneeLocator

x = range(1, len(pca.explained_variance_)+1)
knee = KneeLocator(x, pca.explained_variance_, curve='convex', direction='decreasing')

print("Knee at component:", knee.knee)
```

**PC1 explains approximately 1.67% of the variance and PC2 explains approximately 1.41%, for a total of about 3.08%.** 

**From the scree plot and the KneeLocator result, we see that the optimal number of components is around 30–40 (33 specifically).** After this point, each additional component contributes only a small incremental amount of variance.

Therefore, if the goal is dimensionality reduction while retaining the main structures in the data, I would select roughly 30–40 principal components. However, to capture a large fraction of total variance (such as 80%), many more components would be required (around 200+), which may not be practical for modeling. 


\newpage


# Problem 2
## 2(a)
```{python}
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import train_test_split

X_model = X_pca   # from Problem 1
y_model = y

X_train, X_test, y_train, y_test = train_test_split(
    X_model,
    y_model,
    test_size=0.3,
    random_state=42,
    stratify=y_model
)

C_grid = np.logspace(-3, 0, 6)   # Strong to weak regularization

auc_scores = []
coef_paths = []

for i, C in enumerate(C_grid):
    print(f"Fitting model {i+1}/{len(C_grid)} with C={C}")

    model = LogisticRegression(
        penalty="l1",
        C=C,
        solver="saga",
        max_iter=5000,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # Compute test AUC
    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)
    
    auc_scores.append(auc)
    coef_paths.append(model.coef_[0])
```



```{python}
auc_scores = np.array(auc_scores)

best_index = np.argmax(auc_scores)
best_C = C_grid[best_index]

print(f"Optimal regularization parameter (C): {best_C:.4f}")
```



```{python}
best_AUC = auc_scores[best_index]

print(f"Test set AUC of selected classifier: {best_AUC:.4f}")
```


```{python}
coef_paths = np.array(coef_paths)

plt.figure(figsize=(10,6))

for j in range(coef_paths.shape[1]):
    plt.plot(np.log10(C_grid), coef_paths[:, j], alpha=0.5)

plt.xlabel("log10(C)")
plt.ylabel("Coefficient Value")
plt.title("L1 Logistic Regression Coefficient Paths")
plt.axhline(0, color="black", linewidth=0.8)
plt.grid(alpha=0.3)

plt.show()
```



\newpage

# Problem 3 

## 3(a) 
```{python}
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

X_full = epi.copy()

# Scale all features
scaler_full = StandardScaler()
X_full_scaled = scaler_full.fit_transform(X_full)

K_values = range(2, 18)
within_cluster_var = []

for K in K_values:
    kmeans = KMeans(
        n_clusters=K,
        random_state=42,
        n_init=20
    )
    
    kmeans.fit(X_full_scaled)
    within_cluster_var.append(kmeans.inertia_)  # inertia = WCSS
```


### 3(b)
```{python}
# Elbow plot
plt.figure(figsize=(8,5))
plt.plot(K_values, within_cluster_var, marker='o')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.title("Elbow Plot for K-Means Clustering")
plt.grid(alpha=0.3)
plt.show()
```


```{python}
best_K = 6 

kmeans_final = KMeans(
    n_clusters=best_K,
    random_state=42,
    n_init=20
)

cluster_labels = kmeans_final.fit_predict(X_full_scaled)   
```

The elbow plot does not show a sharp bend. The within-cluster variance decreases smoothly as K increases. However, the rate of decrease slows after approximately K = 6. After this, adding more clusters results in only small reductions in within-cluster variance. So, K = 6 was selected. 


## 3(c)
```{python}
from sklearn.decomposition import PCA

pca_all = PCA()
X_pca_all = pca_all.fit_transform(X_full_scaled)
explained_var = pca_all.explained_variance_ratio_

pc_pairs = [(0,1), (2,3), (4,5), (6,7), (8,9)]

fig, axes = plt.subplots(3, 2, figsize=(15,18))
axes = axes.flatten()

for i, (pc_x, pc_y) in enumerate(pc_pairs):
    scatter = axes[i].scatter(
        X_pca_all[:, pc_x],
        X_pca_all[:, pc_y],
        c=cluster_labels,
        cmap="tab10",
        alpha=0.5,
        s=4
    )
    
    axes[i].set_xlabel(f"PC{pc_x+1} ({explained_var[pc_x]*100:.2f}% var)")
    axes[i].set_ylabel(f"PC{pc_y+1} ({explained_var[pc_y]*100:.2f}% var)")
    axes[i].set_title(f"PC{pc_x+1} vs PC{pc_y+1}")
    
    legend = axes[i].legend(*scatter.legend_elements(), title="Cluster")
    axes[i].add_artist(legend)

fig.delaxes(axes[5])

plt.tight_layout()
plt.show()
```


We see that PC1 explains about $1.66\%$ of the variance and PC2 explains about $1.42\%$. These values are small, but that makes sense since the dataset has many features and the variance is spread out across them.

In the PC1 vs PC2 plot, one cluster is clearly separated along PC1, but the others overlap quite a bit. This suggests that the clusters are not fully separated in just the first two components and that the structure exists across multiple dimensions.



## 3(d) 
We are adding the cluster labels back to the original dataset and then calculating the average feature values for each cluster. This allows us to understand the characteristics of each cluster by looking at which features have higher or lower average values within that cluster compared to others. 
```{python}
epi_with_clusters = epi.copy()
epi_with_clusters["cluster"] = cluster_labels

# Average feature values per cluster
cluster_summary = epi_with_clusters.groupby("cluster").mean()

cluster_summary.head()
```

```{python}
for c in range(best_K):
    print(f"\nCluster {c}")
    print(cluster_summary.loc[c].sort_values(ascending=False).head(10))
```

**Interpretation of Clusters:** 

**Cluster 0 seems to represent savory main dishes**, since it has relatively high sodium, calories, and protein, along with tags like ham. **Cluster 1 is clearly alcoholic drinks**, given the very high drink and alcoholic tags and low protein and fat. **Cluster 2 looks like more everyday or lighter meals**, with moderate nutrition values and tags such as "quick and easy" and "vegetarian". **Cluster 3 appears to be desserts**, since the dessert tag is very high and the nutrition profile fits baked goods. **Cluster 4** stands out as very high in calories, fat, and protein, suggesting **large or rich dinner entrées, or high-protein dishes**. Finally, **Cluster 5 seems to group together dietary-restriction recipes**, with very high peanut-free, soy-free, and pescatarian tags. Overall, most of the clusters are fairly interpretable, even though there is still some overlap between them.