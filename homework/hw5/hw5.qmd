---
title: "DATA 221 Homework 5"
author: "Chris Low"
format: pdf
execute:
  engine: python
  python: /Users/chrislowzx/miniconda3/bin/python
---


# Problem 1

### Exploratory Data Analysis
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

epi = pd.read_csv("epi_r.csv", index_col='title')

# EDA
print(f"Dataset shape: {epi.shape}")
print(epi.info())

nutritional_vars = ['calories', 'protein', 'fat', 'sodium']
print(epi[nutritional_vars].describe())

# Plotting to see the extreme outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=epi[nutritional_vars])
plt.title("Nutritional Variables (Notice the extreme outliers)")
plt.yscale('log') # Log scale: outliers are massive
plt.show()

# Check for missing values
print("Missing values before imputation:\n", epi[nutritional_vars].isna().sum())
```


I decided to pre-process mainly nutritional variables because they are numerical and directly relevant to the analysis we will be doing (PCA and regularized logistic regression). These variables are likely to have a significant impact on the outcome variable 'cake', which is what we will be trying to predict.

The other features, such as Inredients, are not numerical and would require different pre-processing steps (like encoding) if we were to include them in the analysis. For the scope of this homework, we will focus on the nutritional variables as features for PCA and regularized logistic regression, and 'cake' as the outcome variable.


Some pre-processing steps I will take include:

1. Handling Missing Values: I will impute missing values using the median, as it is more robust to outliers than the mean.

2. Handling Outliers: I will cap the values at the 99th percentile to mitigate the influence of extreme outliers without dropping too much data.

3. Scaling the Features: I will standardize the features to have a mean of 0 and a standard deviation of 1, which is crucial for PCA and regularized logistic regression to perform well.

```{python}
# PRE-PROCESSING

# Handle Missing Values (Median: as it is more robust to outliers than mean)
for col in nutritional_vars:
    epi[col] = epi[col].fillna(epi[col].median())

# Handle Outliers (Capping at the 99th percentile). We do this so we don't
# drop too much data, but we eliminate non-sensical values that are likely
# data entry errors or extreme outliers that could skew our analysis.
for col in nutritional_vars:
    cap_value = epi[col].quantile(0.99)
    epi[col] = np.where(epi[col] > cap_value, cap_value, epi[col])

epi[nutritional_vars].skew()

for col in nutritional_vars:
    print(f"{col} skew before log:", epi[col].skew())

    log_col = np.log1p(epi[col])
    print(f"{col} skew after log:", log_col.skew())
    print()
```

From the above code, the nutritional variables still show strong right skew ($\text{skewness} \approx 3$) after capping the 99 percentile. After applying a $log(1 + x)$ transformation, skewness decreased a lot toward zero, so this shows more symmetric distribution. This log transformation prior to scaling improves the stability of PCA.

Moving onto the next steps, where are remove binary tags that are too rare (less than 20 occurrences) (because they are not informative for modeling) and then separate the outcome variable 'cake' from the features, and finally scale the features for PCA and regularized logistic regression.

```{python}
# Remove rare binary tags
binary_cols = [col for col in epi.columns 
               if set(epi[col].unique()).issubset({0,1})]

min_count = 20
rare_cols = [col for col in binary_cols 
             if epi[col].sum() < min_count]

epi = epi.drop(columns=rare_cols)

# Separate outcome (i.e. cake) from features
y = epi['cake']
X = epi.drop(columns=['cake'])

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```


## 1(a) 
```{python}
from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X_scaled)
```


## 1(b)
```{python}
# Transpose because pca.components_ has shape (n_components, n_features),
# and we want (n_features, n_components) to align with our original features
loadings = pca.components_.T

pc1 = loadings[:, 0]
pc2 = loadings[:, 1]

plt.figure(figsize=(12,8))
plt.scatter(pc1, pc2, alpha=0.5)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Loadings: PC1 vs PC2')

# Label strongest contributors to PC1 and PC2 (absolute loading > 0.1)
for i, feature in enumerate(X.columns):
    if abs(pc1[i]) > 0.1 or abs(pc2[i]) > 0.1:
        plt.text(pc1[i], pc2[i], feature, fontsize=8)

plt.show()
```


**Patterns:** 


In the scatter plot of PC1 vs PC2, **most ingredient tags are clustered near the origin**, which suggests they do not strongly influence the first two principal components. This makes sense because many tags are sparse and do not vary consistently across recipes, so they do not drive the main directions of variation.

**The nutritional variables (calories, fat, sodium, and protein) appear grouped together in the lower-left region.** This suggests that one of the components is capturing something like overall richness or heaviness, since these variables tend to increase together. We also see that **alcohol (wine, beer, liquor) tags cluster together in the upper-left area**, which may indicate that they contribute to a different dimension of variation related to alcoholic content or recipe type.

**On the positive side of PC1, dietary restriction tags such as vegetarian, vegan, gluten-free, and dairy-free cluster together,** which suggests that PC1 may represent a dietary or health-conscious dimension.

**Dessert stands out in the upper-right area of the plot,** indicating that it contributes differently from both the nutritional variables and alcohol-related tags. 

Overall, the plot shows clear groupings of related features, showing that the first two principal components capture meaningful structure in the dataset rather than random noise.


## 1(c)
```{python}
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

print("Number of components:", len(pca.explained_variance_ratio_))
print("First 10 explained variances:")
print(explained_variance[:10])

print("Cumulative variance (first 10):")
print(cumulative_variance[:10])

print("Explained by PC1:", explained_variance[0])
print("Explained by PC2:", explained_variance[1])
print("Total explained by PC1 + PC2:", explained_variance[0] + explained_variance[1])
```


```{python}
plt.figure(figsize=(8,5))
plt.plot(cumulative_variance)
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Variance Explained")
plt.title("Cumulative Variance Explained by PCA")
plt.grid(alpha=0.3)
plt.show()
```

Scree plot: 
```{python}
plt.figure(figsize=(8,5))

plt.plot(range(1, len(pca.explained_variance_) + 1),
         pca.explained_variance_,
         marker='o')

plt.xlabel("Principal Component")
plt.ylabel("Eigenvalue")
plt.title("Scree Plot")
plt.grid(alpha=0.3)

plt.show()

```


Based on the scree plot, I cannot see a clear "elbow". However, there is a noticeable drop in eigenvalues after the first few components. I decided to use the \href{https://kneed.readthedocs.io/en/stable/}{KneeLocator} method to identify the optimal number of components. 

```{python}
from kneed import KneeLocator

x = range(1, len(pca.explained_variance_)+1)
knee = KneeLocator(x, pca.explained_variance_, curve='convex', direction='decreasing')

print("Knee at component:", knee.knee)
```

**PC1 explains approximately 1.67% of the variance and PC2 explains approximately 1.41%, for a total of about 3.08%.** 

**From the scree plot and the KneeLocator result, we see that the optimal number of components is around 30–40 (33 specifically).** After this point, each additional component contributes only a small incremental amount of variance.

Therefore, if the goal is dimensionality reduction while retaining the main structures in the data, I would select roughly 30–40 principal components. However, to capture a large fraction of total variance (such as 80%), many more components would be required (around 200+), which may not be practical for modeling. 


\newpage


# Problem 2
## 2(a)
```{python}
from sklearn.linear_model import LogisticRegressionCV

# Define grid of C values
C_grid = np.logspace(-4, 2, 10)

# 2(a) Fit L1-regularized logistic regression with cross-validation
logreg_cv = LogisticRegressionCV(
    Cs=C_grid,
    penalty="l1",
    solver="saga",
    cv=5,
    scoring="roc_auc",
    max_iter=1000,
    random_state=42,
    n_jobs=-1
)

logreg_cv.fit(X_train, y_train)
```


## 2(b)
```{python}
best_C = logreg_cv.C_[0]
print(f"Optimal regularization parameter (C): {best_C:.4f}")
```


## 2(c)
We need to find the test set AUC for classifier with optimal C
```{python}
y_test_prob = logreg_cv.predict_proba(X_test)[:, 1]
test_auc = roc_auc_score(y_test, y_test_prob)

print(f"Test set AUC of selected classifier: {test_auc:.4f}")
```


## 2(d) 
```{python}
coef_paths = []

for C in C_grid:
    model = LogisticRegression(
        penalty="l1",
        C=C,
        solver="saga",
        max_iter=1000,
        random_state=42
    )
    model.fit(X_train, y_train)
    coef_paths.append(model.coef_[0])

coef_paths = np.array(coef_paths)

plt.figure(figsize=(10,6))
for j in range(coef_paths.shape[1]):
    plt.plot(np.log10(C_grid), coef_paths[:, j], alpha=0.5)

plt.xlabel("log10(C)")
plt.ylabel("Coefficient Value")
plt.title("L1 Logistic Regression Coefficient Paths")
plt.axhline(0, color="black", linewidth=0.8)
plt.grid(alpha=0.3)
plt.show()
```




```{python}
# 2(a) Fit L1-regularized logistic regression across a grid of C values

C_grid = np.logspace(-4, 2, 10)   # Strong to weak regularization

auc_scores = []
coef_paths = []

for C in C_grid:
    model = LogisticRegression(
        penalty="l1",
        C=C,
        solver="saga",
        max_iter=5000,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # Compute test AUC
    y_prob = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_prob)
    
    auc_scores.append(auc)
    coef_paths.append(model.coef_[0])
```



```{python}
# 2(b) Select optimal C
auc_scores = np.array(auc_scores)

best_index = np.argmax(auc_scores)
best_C = C_grid[best_index]

print(f"Optimal regularization parameter (C): {best_C:.4f}")
```



```{python}
# 2(c) Test set AUC of selected classifier
best_AUC = auc_scores[best_index]

print(f"Test set AUC of selected classifier: {best_AUC:.4f}")
```


```{python}
# 2(d) Plot coefficient paths

coef_paths = np.array(coef_paths)

plt.figure(figsize=(10,6))

for j in range(coef_paths.shape[1]):
    plt.plot(np.log10(C_grid), coef_paths[:, j], alpha=0.5)

plt.xlabel("log10(C)")
plt.ylabel("Coefficient Value")
plt.title("L1 Logistic Regression Coefficient Paths")
plt.axhline(0, color="black", linewidth=0.8)
plt.grid(alpha=0.3)

plt.show()
```